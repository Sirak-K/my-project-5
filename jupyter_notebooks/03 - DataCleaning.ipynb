{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aStgWSO0E0E"
   },
   "source": [
    "# 03 - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eLEkw5O0ECa"
   },
   "source": [
    "## Objectives\n",
    "\n",
    "Generate a pipeline that performs the data cleaning\n",
    "* Cleaning: Handle Missing Data\n",
    "* Cleaning: Encode categorical variables\n",
    "\n",
    "## Inputs\n",
    "\n",
    "outputs/datasets/collection/house_prices_records.csv\n",
    "\n",
    "## Outputs\n",
    "\n",
    "\n",
    "\n",
    "## Additional Notes\n",
    "### Data Cleaning - Imputation\n",
    "##### Target: Missing Values\n",
    "##### Used Strategies:\n",
    "- Drop\n",
    "- Median"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cqP-UeN-z3i2"
   },
   "source": [
    "# Change working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wZfF_j-Bz3i4",
    "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwHsQRWjz3i9",
    "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
   },
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vz3S-_kjz3jA",
    "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
   },
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mavJ8DibrcQ"
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataset_raw_path = \"outputs/datasets/collection/house_prices_records.csv\"\n",
    "dataset = pd.read_csv(dataset_raw_path)\n",
    "dataset.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Encode Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Mapping specific categories of categorical variables to numerical values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_for_encoding = {\n",
    "'BsmtExposure': {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'None': 0},\n",
    "'BsmtFinType1': {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'None': 0}, 'GarageFinish': {'Fin': 3, 'RFn': 2, 'Unf': 1, 'None': 0}, \n",
    "'KitchenQual': {'Ex': 4, 'Gd': 3, 'TA': 2, 'Fa': 1, 'Po': 0}\n",
    "}\n",
    "\n",
    "\n",
    "dataset_2=dataset.copy()\n",
    "for col in dataset.columns[dataset.dtypes=='object'].to_list():\n",
    "    dataset_2[col] = dataset_2[col].replace(dict_for_encoding[col])\n",
    "dataset_2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a custom transformer for encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "# Create the Class\n",
    "class MyCustomEncoder(BaseEstimator, TransformerMixin):\n",
    "\n",
    "  def __init__(self, variables, dict_for_encoding):\n",
    "    if not isinstance(variables, list): \n",
    "      self.variables = [variables]\n",
    "    else: self.variables = variables\n",
    "    self.dict_for_encoding = dict_for_encoding\n",
    "\n",
    "  def fit(self, X, y=None):    \n",
    "    return self\n",
    "\n",
    "  def transform(self, X):\n",
    "    for col in self.variables:\n",
    "      if X[col].dtype == 'object':\n",
    "        X[col] = X[col].replace(dict_for_encoding[col])\n",
    "      else:\n",
    "        print(f\"Warning: {col} the Data Type has to be an object\")\n",
    "      \n",
    "    return X\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipeline = Pipeline([('custom_encoder', MyCustomEncoder(variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'], dict_for_encoding=dict_for_encoding))])\n",
    "\n",
    "dataset_2 = dataset.copy()\n",
    "dataset_2 = pipeline.fit_transform(dataset_2)\n",
    "dataset_2.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning - Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom function to display missing data levels in a DataFrames, it shows:\n",
    "- Absolute levels\n",
    "- Relative levels\n",
    "- Data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluateMissingData(dataset):\n",
    "    missing_data_absolute = dataset.isnull().sum()\n",
    "    missing_data_percentage = round(missing_data_absolute/len(dataset)*100, 2)\n",
    "    dataset_missing_data = (pd.DataFrame(\n",
    "                            data={\"RowsWithMissingData\": missing_data_absolute,\n",
    "                                   \"PercentageOfDataset\": missing_data_percentage,\n",
    "                                   \"DataType\": dataset.dtypes}\n",
    "                                    )\n",
    "                          .sort_values(by=['PercentageOfDataset'], ascending=False)\n",
    "                          .query(\"PercentageOfDataset > 0\")\n",
    "                          )\n",
    "\n",
    "    return dataset_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check missing data levels for the collected dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EvaluateMissingData(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Features with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vars_with_missing_data = dataset.columns[dataset.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputations for Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imputation - Drop Two Variables with approx. 90% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "variables_method = ['WoodDeckSF', 'EnclosedPorch' ]\n",
    "\n",
    "print(f\"* {len(variables_method)} features to drop \\n\\n\"\n",
    "    f\"{variables_method}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.selection import DropFeatures\n",
    "\n",
    "drop_features = DropFeatures(features_to_drop=['EnclosedPorch', 'WoodDeckSF'])\n",
    "\n",
    "dataset_for_drop = drop_features.fit_transform(dataset)\n",
    "dataset_for_drop.info()\n",
    "\n",
    "# Check if variables have been dropped\n",
    "if 'EnclosedPorch' not in dataset_for_drop.columns and 'WoodDeckSF' not in dataset_for_drop.columns:\n",
    "    print(\"Variables 'WoodDeckSF' and 'EnclosedPorch' have been successfully dropped.\")\n",
    "else:\n",
    "    print(\"Variables could not be dropped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check the list of features after dropping\n",
    "EvaluateMissingData(dataset_for_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the two dropped features from vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.imputation import MeanMedianImputer\n",
    "vars_with_missing_data = ['2ndFlrSF', 'BedroomAbvGr', 'BsmtFinType1', 'GarageFinish', 'GarageYrBlt', 'LotFrontage', 'MasVnrArea']\n",
    "\n",
    "pipeline = Pipeline([\n",
    "      ('drop_features', DropFeatures(features_to_drop = ['EnclosedPorch', 'WoodDeckSF'])),\n",
    "      ('custom_encoder', MyCustomEncoder(variables=['BsmtExposure', 'BsmtFinType1', 'GarageFinish', 'KitchenQual'], dict_for_encoding=dict_for_encoding)),\n",
    "      ('median_imputer',  MeanMedianImputer(imputation_method='median', variables=vars_with_missing_data))\n",
    "])\n",
    "\n",
    "dataset_2 = dataset.copy()\n",
    "dataset_transformed = pipeline.fit_transform(dataset_2) \n",
    "dataset_transformed.head(5)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Double-Check Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = dataset_transformed.columns[dataset_transformed.isna().sum() > 0].to_list()\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confirm Pipeline : Median Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['median_imputer'].imputer_dict_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation and PPS Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ppscore as pps\n",
    "\n",
    "\n",
    "def heatmap_corr(dataset, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(dataset.columns) > 1:\n",
    "        mask = np.zeros_like(dataset, dtype=np.bool)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        mask[abs(dataset) < threshold] = True\n",
    "\n",
    "        fig, axes = plt.subplots(figsize=figsize)\n",
    "        sns.heatmap(dataset, annot=True, xticklabels=True, yticklabels=True,\n",
    "                    mask=mask, cmap='viridis', annot_kws={\"size\": font_annot}, ax=axes,\n",
    "                    linewidth=0.5\n",
    "                    )\n",
    "        axes.set_yticklabels(dataset.columns, rotation=0)\n",
    "        plt.ylim(len(dataset.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def heatmap_pps(dataset, threshold, figsize=(20, 12), font_annot=8):\n",
    "    if len(dataset.columns) > 1:\n",
    "        mask = np.zeros_like(dataset, dtype=np.bool)\n",
    "        mask[abs(dataset) < threshold] = True\n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        ax = sns.heatmap(dataset, annot=True, xticklabels=True, yticklabels=True,\n",
    "                         mask=mask, cmap='rocket_r', annot_kws={\"size\": font_annot},\n",
    "                         linewidth=0.05, linecolor='grey')\n",
    "        plt.ylim(len(dataset.columns), 0)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def CalculateCorrAndPPS(dataset):\n",
    "    dataset_corr_spearman = dataset.corr(method=\"spearman\")\n",
    "    dataset_corr_pearson = dataset.corr(method=\"pearson\")\n",
    "\n",
    "    pps_matrix_raw = pps.matrix(dataset)\n",
    "    pps_matrix = pps_matrix_raw.filter(['x', 'y', 'ppscore']).pivot(columns='x', index='y', values='ppscore')\n",
    "\n",
    "    pps_score_stats = pps_matrix_raw.query(\"ppscore < 1\").filter(['ppscore']).describe().T\n",
    "    print(\"PPS threshold - check PPS score IQR to decide threshold for heatmap \\n\")\n",
    "    print(pps_score_stats.round(3))\n",
    "\n",
    "    return dataset_corr_pearson, dataset_corr_spearman, pps_matrix\n",
    "\n",
    "\n",
    "def DisplayCorrAndPPS(dataset_corr_pearson, dataset_corr_spearman, pps_matrix, CorrThreshold, PPS_Threshold,\n",
    "                      figsize=(20, 12), font_annot=8):\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"* Analyse how the target variable for your ML models are correlated with other variables (features and target)\")\n",
    "    print(\"* Analyse multi-colinearity, that is, how the features are correlated among themselves\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Spearman Correlation ***\")\n",
    "    print(\"It evaluates monotonic relationship \\n\")\n",
    "    heatmap_corr(dataset=dataset_corr_spearman, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Pearson Correlation ***\")\n",
    "    print(\"It evaluates the linear relationship between two continuous variables \\n\")\n",
    "    heatmap_corr(dataset=dataset_corr_pearson, threshold=CorrThreshold, figsize=figsize, font_annot=font_annot)\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"*** Heatmap: Power Predictive Score (PPS) ***\")\n",
    "    print(f\"PPS detects linear or non-linear relationships between two columns.\\n\"\n",
    "          f\"The score ranges from 0 (no predictive power) to 1 (perfect predictive power) \\n\")\n",
    "    heatmap_pps(dataset=pps_matrix, threshold=PPS_Threshold, figsize=figsize, font_annot=font_annot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Correlations and Power Predictive Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset_corr_pearson, dataset_corr_spearman, pps_matrix = CalculateCorrAndPPS(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display at Heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DisplayCorrAndPPS(dataset_corr_pearson = dataset_corr_pearson,\n",
    "                  dataset_corr_spearman = dataset_corr_spearman, \n",
    "                  pps_matrix = pps_matrix,\n",
    "                  CorrThreshold = 0.4, PPS_Threshold =0.2,\n",
    "                  figsize=(12,10), font_annot=10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Data Practitioner Jupyter Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
   "language": "python",
   "name": "python381264bit3812pyenv81cdd682ff264c619337b331e5d72cb4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
